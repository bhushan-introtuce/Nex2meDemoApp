package co.introtuce.nex2me.test.rtc.custom;

import android.app.Activity;
import android.content.Context;
import android.graphics.Bitmap;
import android.graphics.SurfaceTexture;
import android.util.Log;
import android.view.Surface;
import android.view.TextureView;
import android.view.View;

import com.google.mediapipe.components.PermissionHelper;
import com.google.mediapipe.framework.AndroidAssetUtil;
import com.google.mediapipe.framework.Packet;
import com.google.mediapipe.framework.PacketCallback;
import com.google.mediapipe.glutil.EglManager;

import co.introtuce.nex2me.test.analytics.MpExternalTextureConverter;
import co.introtuce.nex2me.test.ui.videoviews.AutoFitTextureView;


public class MediapipeController implements Nex2meSegmenter {
    private static final String TAG = "MediapipeController";
    //private static final String BINARY_GRAPH_NAME = "segmentationgraph.binarypb";
    private static final String BINARY_GRAPH_NAME = "hairsegmentationgpu.binarypb";
    private static final String INPUT_VIDEO_STREAM_NAME = "input_video";
    private static  final String bGVideoInputStream = "bg_video";
    private static final String OUTPUT_VIDEO_STREAM_NAME = "output_video";
    private static  final String x_center = "x_center";
    private static  final String y_center = "y_center";


   StringBuilder stringBuilder = new StringBuilder();
    // Flips the camera-preview frames vertically before sending them into FrameProcessor to be
    // processed in a MediaPipe graph, and flips the processed frames back when they are displayed.
    // This is needed because OpenGL represents images assuming the image origin is at the bottom-left
    // corner, whereas MediaPipe in general assumes the image origin is at top-left.
    private static final boolean FLIP_FRAMES_VERTICALLY = true;
    // Creates and manages an {@link EGLContext}.
    private EglManager eglManager;
    // Sends camera-preview frames into a MediaPipe graph for processing, and displays the processed
    // frames onto a {@link Surface}.
    private CustomFrameProcessor processor;
    // Converts the GL_TEXTURE_EXTERNAL_OES texture from Android camera into a regular texture to be
    // consumed by {@link FrameProcessor} and the underlying MediaPipe graph.
    private AutoFitTextureView videoTexture;
    private MpExternalTextureConverter converter;
    private int mWidth=0,mHeight=0;
    private Activity context;
    public MediapipeController(Activity context){
        this.context=context;
    }
    static {
        // Load all native libraries needed by the app.
        System.loadLibrary("mediapipe_jni");
        try {
            System.loadLibrary("opencv_java3");
        } catch (java.lang.UnsatisfiedLinkError e) {
            // Some example apps (e.g. template matching) require OpenCV 4.
            System.loadLibrary("opencv_java4");
        }
    }
    private AutoFitTextureView outputTexture;

    public void initMediapipe(AutoFitTextureView videoTexture){
        //videoTexture.getSurfaceTexture().detachFromGLContext();
        this.outputTexture = videoTexture;
        setupPreviewDisplayView(videoTexture);
        //previewDisplayView = new SurfaceView(this);
        //setupPreviewDisplayView();
        // Initialize asset manager so that MediaPipe native libraries can access the app assets, e.g.,
        // binary graphs.
        AndroidAssetUtil.initializeNativeAssetManager((Context) context);
        eglManager = new EglManager(null);
        processor =
                new CustomFrameProcessor(
                        context,
                        eglManager.getNativeContext(),
                        BINARY_GRAPH_NAME,
                        INPUT_VIDEO_STREAM_NAME,
                        OUTPUT_VIDEO_STREAM_NAME,bGVideoInputStream,x_center,y_center);
        processor.getVideoSurfaceOutput().setFlipY(FLIP_FRAMES_VERTICALLY);
        //FrameProcessor cf = (FrameProcessor)processor;
        //cf.setbGVideoInputStream(bGVideoInputStream);
        processor.getGraph().addPacketCallback(OUTPUT_VIDEO_STREAM_NAME, new PacketCallback() {
            @Override
            public void process(Packet packet) {
                Log.d(TAG,"new output");
                long currentTIme = System.currentTimeMillis();
                if(oldTime!=0l){
                    long runtime = (currentTIme-oldTime);
                    if(stringBuilder != null){
                        stringBuilder.append("\nRuntime : "+(runtime));
                    }
                }
                oldTime=currentTIme;
            }
        });
        PermissionHelper.checkAndRequestCameraPermissions(context);
    }
    long oldTime=0l;
    @Override
    public void resume() {
        converter = new MpExternalTextureConverter(eglManager.getContext());
        //converter.setFlipY(FLIP_FRAMES_VERTICALLY);
        converter.setConsumer(processor);
        if (PermissionHelper.cameraPermissionsGranted(context)) {
            setFrameReceiveMode(true);
            try{
                startGraph();
            }catch (Exception e){
                Log.d(TAG,e.toString());
            }
        }

    }

    @Override
    public void pause() {
        frameReceiveMode = false;
        converter.close();

    }

    private void setupPreviewDisplayView(AutoFitTextureView textureView){
        textureView.setVisibility(View.GONE);
        textureView.setSurfaceTextureListener(new TextureView.SurfaceTextureListener() {
            @Override
            public void onSurfaceTextureAvailable(SurfaceTexture surface, int width, int height) {
                Log.d(TAG,"onSurfaceTextureAvailable(SurfaceTexture surface, int width, int height)");
                Surface surface1 = new Surface(surface);
                mHeight=height;
                mWidth=width;
                processor.getVideoSurfaceOutput().setSurface(surface1);
                //converter.setSurfaceTextureAndAttachToGLContext(previewFrameTexture,width,height);
                //converter.setbGSurfaceTextureAndAttachToGLContext(surfaceTexture,width,height);
                //converter.setbGSurfaceTextureAndAttachToGLContext(previewFrameTexture,width,height);
                //converter.setSurfaceTextureAndAttachToGLContext(surfaceTexture,width,height);
                //initRecorder();
            }

            @Override
            public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {
                Log.d(TAG,"onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height)");
            }

            @Override
            public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {
                Log.d(TAG,"oonSurfaceTextureDestroyed(SurfaceTexture surface)");
                processor.getVideoSurfaceOutput().setSurface(null);
                return false;
            }

            @Override
            public void onSurfaceTextureUpdated(SurfaceTexture surface) {
                //Log.d(TAG,"onSurfaceTextureUpdated(SurfaceTexture surface)");

            }
        });
    }


    private boolean frameReceiveMode=false;
    @Override
    public void setFrameReceiveMode(boolean frameReceiveMode) {
        this.frameReceiveMode = frameReceiveMode;
    }

    @Override
    public void onForgroundFrame(Bitmap bitmap) {

        if(frameReceiveMode && converter!=null){
            //Log.d(TAG,"onForgroundFrame(Bitmap bitmap)");
            converter.onFrame(bitmap);
        }
    }

    @Override
    public void onBackGroundFrame(Bitmap bitmap) {
        if(frameReceiveMode && converter!=null){
            converter.onBGFrame(bitmap);
        }
    }

    @Override
    public void startGraph() throws NoOutputTextureDefineException{
        Log.d(TAG,"Starting graph");
        if(outputTexture!=null){
            outputTexture.setVisibility(View.VISIBLE);
        }
        else {
            throw new NoOutputTextureDefineException("No output surface found");
        }

    }

    public StringBuilder getRuntimeLog(){
        return stringBuilder;
    }
}